{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad1dbcc-28e0-45f9-be2d-f8b745e6a16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def generate_synthetic_samples_imbalanced(X, y, n_samples, threshold=0.2, imbalance_factor=2, random_state=42):\\n    np.random.seed(random_state)\\n    nn = NearestNeighbors(n_neighbors=5)\\n    nn.fit(X)\\n\\n    synthetic_X = []\\n    synthetic_y = []\\n\\n    # Calculate the distance to the nearest neighbors\\n    distances, neighbors = nn.kneighbors(X)\\n\\n    # Identify samples with fewer neighbors within the threshold distance\\n    low_density_indices = [i for i, dist in enumerate(distances[:, 1:]) if np.sum(dist < threshold) < 3]\\n\\n    # Increase the duplication factor for imbalanced samples\\n    for _ in range(n_samples):\\n        # Select a sample, with preference to low-density samples\\n        if low_density_indices and np.random.rand() < 0.5:\\n            idx = np.random.choice(low_density_indices)\\n        else:\\n            idx = np.random.randint(0, len(X))\\n\\n        neighbor_idxs = neighbors[idx]\\n        neighbor_idx = np.random.choice(neighbor_idxs)\\n\\n        # Linear interpolation for synthetic sample generation\\n        lam = np.random.uniform(0, 1)\\n        new_sample_X = X[idx] + lam * (X[neighbor_idx] - X[idx])\\n        new_sample_y = y[idx] + lam * (y[neighbor_idx] - y[idx])\\n\\n        synthetic_X.append(new_sample_X)\\n        synthetic_y.append(new_sample_y)\\n\\n        # Additional synthetic samples for imbalanced samples\\n        if idx in low_density_indices:\\n            for _ in range(imbalance_factor - 1):\\n                neighbor_idx = np.random.choice(neighbor_idxs)\\n                lam = np.random.uniform(0, 1)\\n                new_sample_X = X[idx] + lam * (X[neighbor_idx] - X[idx])\\n                new_sample_y = y[idx] + lam * (y[neighbor_idx] - y[idx])\\n\\n                synthetic_X.append(new_sample_X)\\n                synthetic_y.append(new_sample_y)\\n\\n    return np.array(synthetic_X), np.array(synthetic_y)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def generate_synthetic_samples_imbalanced(X, y, n_samples, threshold=0.2, imbalance_factor=2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    nn = NearestNeighbors(n_neighbors=5)\n",
    "    nn.fit(X)\n",
    "\n",
    "    synthetic_X = []\n",
    "    synthetic_y = []\n",
    "\n",
    "    # Calculate the distance to the nearest neighbors\n",
    "    distances, neighbors = nn.kneighbors(X)\n",
    "\n",
    "    # Identify samples with fewer neighbors within the threshold distance\n",
    "    low_density_indices = [i for i, dist in enumerate(distances[:, 1:]) if np.sum(dist < threshold) < 3]\n",
    "\n",
    "    # Increase the duplication factor for imbalanced samples\n",
    "    for _ in range(n_samples):\n",
    "        # Select a sample, with preference to low-density samples\n",
    "        if low_density_indices and np.random.rand() < 0.5:\n",
    "            idx = np.random.choice(low_density_indices)\n",
    "        else:\n",
    "            idx = np.random.randint(0, len(X))\n",
    "\n",
    "        neighbor_idxs = neighbors[idx]\n",
    "        neighbor_idx = np.random.choice(neighbor_idxs)\n",
    "\n",
    "        # Linear interpolation for synthetic sample generation\n",
    "        lam = np.random.uniform(0, 1)\n",
    "        new_sample_X = X[idx] + lam * (X[neighbor_idx] - X[idx])\n",
    "        new_sample_y = y[idx] + lam * (y[neighbor_idx] - y[idx])\n",
    "\n",
    "        synthetic_X.append(new_sample_X)\n",
    "        synthetic_y.append(new_sample_y)\n",
    "\n",
    "        # Additional synthetic samples for imbalanced samples\n",
    "        if idx in low_density_indices:\n",
    "            for _ in range(imbalance_factor - 1):\n",
    "                neighbor_idx = np.random.choice(neighbor_idxs)\n",
    "                lam = np.random.uniform(0, 1)\n",
    "                new_sample_X = X[idx] + lam * (X[neighbor_idx] - X[idx])\n",
    "                new_sample_y = y[idx] + lam * (y[neighbor_idx] - y[idx])\n",
    "\n",
    "                synthetic_X.append(new_sample_X)\n",
    "                synthetic_y.append(new_sample_y)\n",
    "\n",
    "    return np.array(synthetic_X), np.array(synthetic_y)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1536f7-7329-4027-ad80-2606c040f62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Optimise test size and number of added samples\\nX = df_vape[[\\'tax_increase\\', \\'outlet_reduction\\', \\'dec_smoking_prevalence\\', \\n              \\'dec_tobacco_supply\\', \\'dec_smoking_uptake\\', \\'average_age\\', \\n              \\'gender_idx\\', \\'ethnicity_idx\\']]\\ny = df_vape[[\\'qalys_pc\\']]\\n\\n# Ensure that y is a 1D array for compatibility\\ny_flat = y.values.flatten()\\n\\n# Hyperparameters to test\\ntest_sizes = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\\nadded_samples = [100, 150, 200, 250, 300]\\n\\n# Best MAPE and hyperparameter values to use\\nrfnb_best_test_size = 0\\nrfnb_best_sam = 0\\nrfnb_best_mape = 1e6\\n\\nrfb_best_test_size = 0\\nrfb_best_sam = 0\\nrfb_best_mape = 1e6\\n\\nxgb_best_test_size = 0\\nxgb_best_sam = 0\\nxgb_best_mape = 1e6\\n\\nfor test_size in test_sizes:\\n    for n_samples in added_samples:\\n        # Generate synthetic samples with fixed random state\\n        X_res, y_res = generate_synthetic_samples(X.values, y_flat, n_samples=n_samples, random_state=42)\\n        \\n        # Stack the original and synthetic data\\n        X_full = np.vstack([X.values, X_res])\\n        y_full = np.hstack([y_flat, y_res])\\n        \\n        # Convert to DataFrame for easier handling\\n        df_resampled = pd.DataFrame(X_full, columns=X.columns)\\n        df_resampled[\\'qalys_pc\\'] = y_full\\n        \\n        X_resampled = df_resampled[[\\'tax_increase\\', \\'outlet_reduction\\', \\'dec_smoking_prevalence\\', \\n              \\'dec_tobacco_supply\\', \\'dec_smoking_uptake\\', \\'average_age\\', \\n              \\'gender_idx\\', \\'ethnicity_idx\\']]\\n        y_resampled = df_resampled[[\\'qalys_pc\\']]\\n\\n        print(f\\'Test Size: {test_size}, Samples added: {n_samples}\\')\\n        _, rfnb_mape = rf_no_bootstrap(X_resampled, y_resampled, test_size)\\n        _, rfb_mape = rf_bootstrap(X_resampled, y_resampled, test_size)\\n        _, xgb_mape = xgboost(X_resampled, y_resampled, test_size)\\n        print(\"-----------------------------\")\\n\\n        if rfnb_mape < rfnb_best_mape:\\n            rfnb_best_mape = rfnb_mape\\n            rfnb_best_sam = n_samples\\n            rfnb_best_test_size = test_size\\n\\n        if rfb_mape < rfb_best_mape:\\n            rfb_best_mape = rfb_mape\\n            rfb_best_sam = n_samples\\n            rfb_best_test_size = test_size\\n\\n        if xgb_mape < xgb_best_mape:\\n            xgb_best_mape = xgb_mape\\n            xgb_best_sam = n_samples\\n            xgb_best_test_size = test_size\\n\\nprint(\"-----------------------------\")\\nprint(\"\")\\nprint(f\\'Random Forest without Bootstrapping - Best MAPE: {rfnb_best_mape}, Best number of samples to add: {rfnb_best_sam}, Best Test Size: {rfnb_best_test_size}\\')\\nprint(f\\'Random Forest with Bootstrapping - Best MAPE: {rfb_best_mape}, Best number of samples to add: {rfb_best_sam}, Best Test Size: {rfb_best_test_size}\\')\\nprint(f\\'XGBoost - Best MAPE: {xgb_best_mape}, Best number of samples to add: {xgb_best_sam}, Best Test Size: {xgb_best_test_size}\\')'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Optimise test size and number of added samples\n",
    "X = df_vape[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "              'dec_tobacco_supply', 'dec_smoking_uptake', 'average_age', \n",
    "              'gender_idx', 'ethnicity_idx']]\n",
    "y = df_vape[['qalys_pc']]\n",
    "\n",
    "# Ensure that y is a 1D array for compatibility\n",
    "y_flat = y.values.flatten()\n",
    "\n",
    "# Hyperparameters to test\n",
    "test_sizes = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "added_samples = [100, 150, 200, 250, 300]\n",
    "\n",
    "# Best MAPE and hyperparameter values to use\n",
    "rfnb_best_test_size = 0\n",
    "rfnb_best_sam = 0\n",
    "rfnb_best_mape = 1e6\n",
    "\n",
    "rfb_best_test_size = 0\n",
    "rfb_best_sam = 0\n",
    "rfb_best_mape = 1e6\n",
    "\n",
    "xgb_best_test_size = 0\n",
    "xgb_best_sam = 0\n",
    "xgb_best_mape = 1e6\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    for n_samples in added_samples:\n",
    "        # Generate synthetic samples with fixed random state\n",
    "        X_res, y_res = generate_synthetic_samples(X.values, y_flat, n_samples=n_samples, random_state=42)\n",
    "        \n",
    "        # Stack the original and synthetic data\n",
    "        X_full = np.vstack([X.values, X_res])\n",
    "        y_full = np.hstack([y_flat, y_res])\n",
    "        \n",
    "        # Convert to DataFrame for easier handling\n",
    "        df_resampled = pd.DataFrame(X_full, columns=X.columns)\n",
    "        df_resampled['qalys_pc'] = y_full\n",
    "        \n",
    "        X_resampled = df_resampled[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "              'dec_tobacco_supply', 'dec_smoking_uptake', 'average_age', \n",
    "              'gender_idx', 'ethnicity_idx']]\n",
    "        y_resampled = df_resampled[['qalys_pc']]\n",
    "\n",
    "        print(f'Test Size: {test_size}, Samples added: {n_samples}')\n",
    "        _, rfnb_mape = rf_no_bootstrap(X_resampled, y_resampled, test_size)\n",
    "        _, rfb_mape = rf_bootstrap(X_resampled, y_resampled, test_size)\n",
    "        _, xgb_mape = xgboost(X_resampled, y_resampled, test_size)\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "        if rfnb_mape < rfnb_best_mape:\n",
    "            rfnb_best_mape = rfnb_mape\n",
    "            rfnb_best_sam = n_samples\n",
    "            rfnb_best_test_size = test_size\n",
    "\n",
    "        if rfb_mape < rfb_best_mape:\n",
    "            rfb_best_mape = rfb_mape\n",
    "            rfb_best_sam = n_samples\n",
    "            rfb_best_test_size = test_size\n",
    "\n",
    "        if xgb_mape < xgb_best_mape:\n",
    "            xgb_best_mape = xgb_mape\n",
    "            xgb_best_sam = n_samples\n",
    "            xgb_best_test_size = test_size\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"\")\n",
    "print(f'Random Forest without Bootstrapping - Best MAPE: {rfnb_best_mape}, Best number of samples to add: {rfnb_best_sam}, Best Test Size: {rfnb_best_test_size}')\n",
    "print(f'Random Forest with Bootstrapping - Best MAPE: {rfb_best_mape}, Best number of samples to add: {rfb_best_sam}, Best Test Size: {rfb_best_test_size}')\n",
    "print(f'XGBoost - Best MAPE: {xgb_best_mape}, Best number of samples to add: {xgb_best_sam}, Best Test Size: {xgb_best_test_size}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59840b9c-2601-4e22-9e9c-2488a0d4dfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
