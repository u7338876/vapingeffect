{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00066f8f-78d3-409f-b084-5c4983b85eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d29fb8-6f8d-46bd-8d72-d87d7bdfc819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intervention_descriptor</th>\n",
       "      <th>tax_increase</th>\n",
       "      <th>outlet_reduction</th>\n",
       "      <th>dec_smoking_prevalence</th>\n",
       "      <th>dec_tobacco_supply</th>\n",
       "      <th>dec_smoking_uptake</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>evidence_strength</th>\n",
       "      <th>qalys_pc</th>\n",
       "      <th>hs_costs_pc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combined tobacco endgame strategy (tobacco-fre...</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>non-Māori</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.865526</td>\n",
       "      <td>-1284765.096725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Combined tobacco endgame strategy (tobacco-fre...</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15-24</td>\n",
       "      <td>Male</td>\n",
       "      <td>non-Māori</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.708939</td>\n",
       "      <td>-1270055.987675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined tobacco endgame strategy (tobacco-fre...</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25-44</td>\n",
       "      <td>Male</td>\n",
       "      <td>non-Māori</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.282615</td>\n",
       "      <td>-318700.524314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined tobacco endgame strategy (tobacco-fre...</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45-64</td>\n",
       "      <td>Male</td>\n",
       "      <td>non-Māori</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.222291</td>\n",
       "      <td>-119003.652181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Combined tobacco endgame strategy (tobacco-fre...</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65+</td>\n",
       "      <td>Male</td>\n",
       "      <td>non-Māori</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.111505</td>\n",
       "      <td>-9656.694651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                            Intervention_descriptor tax_increase  \\\n",
       "1  Combined tobacco endgame strategy (tobacco-fre...           10   \n",
       "2  Combined tobacco endgame strategy (tobacco-fre...           10   \n",
       "3  Combined tobacco endgame strategy (tobacco-fre...           10   \n",
       "4  Combined tobacco endgame strategy (tobacco-fre...           10   \n",
       "5  Combined tobacco endgame strategy (tobacco-fre...           10   \n",
       "\n",
       "0 outlet_reduction dec_smoking_prevalence dec_tobacco_supply  \\\n",
       "1               90                      7                  0   \n",
       "2               90                      7                  0   \n",
       "3               90                      7                  0   \n",
       "4               90                      1                  0   \n",
       "5               90                    0.5                  0   \n",
       "\n",
       "0 dec_smoking_uptake    age gender  ethnicity discount_rate evidence_strength  \\\n",
       "1                  0   0-14   Male  non-Māori             0               NaN   \n",
       "2                  0  15-24   Male  non-Māori             0               NaN   \n",
       "3                  0  25-44   Male  non-Māori             0               NaN   \n",
       "4                  0  45-64   Male  non-Māori             0               NaN   \n",
       "5                  0    65+   Male  non-Māori             0               NaN   \n",
       "\n",
       "0   qalys_pc     hs_costs_pc  \n",
       "1  40.865526 -1284765.096725  \n",
       "2  41.708939 -1270055.987675  \n",
       "3  13.282615  -318700.524314  \n",
       "4   7.222291  -119003.652181  \n",
       "5   1.111505    -9656.694651  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Excel file and save as DataFrame\n",
    "\n",
    "df = pd.read_excel('./Datasets/tobacco_data.xlsx')\n",
    "df.columns = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7372f5-136b-4d26-a5ff-1db7fcdc2794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tax_increase</th>\n",
       "      <th>outlet_reduction</th>\n",
       "      <th>dec_smoking_prevalence</th>\n",
       "      <th>dec_tobacco_supply</th>\n",
       "      <th>dec_smoking_uptake</th>\n",
       "      <th>average_age</th>\n",
       "      <th>gender_idx</th>\n",
       "      <th>ethnicity_idx</th>\n",
       "      <th>qalys_pc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.865526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.708939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.282615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.222291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.111505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  tax_increase  outlet_reduction  dec_smoking_prevalence  dec_tobacco_supply  \\\n",
       "1          10.0              90.0                     7.0                 0.0   \n",
       "2          10.0              90.0                     7.0                 0.0   \n",
       "3          10.0              90.0                     7.0                 0.0   \n",
       "4          10.0              90.0                     1.0                 0.0   \n",
       "5          10.0              90.0                     0.5                 0.0   \n",
       "\n",
       "0  dec_smoking_uptake  average_age  gender_idx  ethnicity_idx   qalys_pc  \n",
       "1                 0.0          7.0         0.0            1.0  40.865526  \n",
       "2                 0.0         20.0         0.0            1.0  41.708939  \n",
       "3                 0.0         33.0         0.0            1.0  13.282615  \n",
       "4                 0.0         55.0         0.0            1.0   7.222291  \n",
       "5                 0.0         75.0         0.0            1.0   1.111505  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform data\n",
    "\n",
    "# Map age group to integer\n",
    "avg_age_mapping = {\n",
    "    '0-14': 7,\n",
    "    '15-24': 20,\n",
    "    '25-44': 33,\n",
    "    '45-64': 55,\n",
    "    '65+': 75\n",
    "}\n",
    "\n",
    "# Map gender to integer\n",
    "gender_mapping = {\n",
    "    'Male': 0,\n",
    "    'Female': 1\n",
    "}\n",
    "\n",
    "# Map ethnicity to integer\n",
    "ethnicity_mapping = {\n",
    "    'Māori': 0,\n",
    "    'non-Māori': 1\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Age_Group' column\n",
    "df['average_age'] = df['age'].map(avg_age_mapping)\n",
    "df['gender_idx'] = df['gender'].map(gender_mapping)\n",
    "df['ethnicity_idx'] = df['ethnicity'].map(ethnicity_mapping)\n",
    "\n",
    "# Impute missing values in 'average_age' with the mean\n",
    "df['average_age'] = df['average_age'].fillna(df['average_age'].mean())\n",
    "\n",
    "# Impute missing values in 'gender_idx' and 'ethnicity_idx' with the mode\n",
    "df['gender_idx'] = df['gender_idx'].fillna(df['gender_idx'].mode()[0])\n",
    "df['ethnicity_idx'] = df['ethnicity_idx'].fillna(df['ethnicity_idx'].mode()[0])\n",
    "\n",
    "# Convert the specified columns to floats\n",
    "df[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "    'dec_tobacco_supply', 'dec_smoking_uptake', 'qalys_pc']] = df[['tax_increase', 'outlet_reduction', \n",
    "    'dec_smoking_prevalence', 'dec_tobacco_supply', 'dec_smoking_uptake', 'qalys_pc']].apply(pd.to_numeric, errors='coerce').astype('float')\n",
    "\n",
    "# Columns to be used for model building\n",
    "df_vape = df[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "              'dec_tobacco_supply', 'dec_smoking_uptake', 'average_age', \n",
    "              'gender_idx', 'ethnicity_idx', 'qalys_pc']]\n",
    "\n",
    "# Display updated DataFrame\n",
    "df_vape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75bf85e-2942-4932-8971-8477fbe3ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for col in df_vape.columns:\n",
    "    print(sum(df_vape.isna()[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16b98c68-5524-419e-93f6-f214caab8aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e74f74-8c71-4f40-9385-9e8613f87afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca(df):\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Check variance ratio for the first two components\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"Explained variance by component 1: {explained_variance[0]:.2f}\")\n",
    "    print(f\"Explained variance by component 2: {explained_variance[1]:.2f}\")\n",
    "    \n",
    "    # Create a scatter plot of the PCA results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c='blue', edgecolor='k', s=50)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(f\"Principal Component 1 ({explained_variance[0]:.2f} variance)\")\n",
    "    plt.ylabel(f\"Principal Component 2 ({explained_variance[1]:.2f} variance)\")\n",
    "    plt.title(\"PCA of Dataset\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36080614-ceae-4272-b388-2d75f9282650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_duplicate(X, y, n_samples=200, random_state=42, noise_std=0.01):\n",
    "    # Set random state for reproducibility\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Determine how many times to duplicate the dataset\n",
    "    n_repeats = n_samples // len(X)\n",
    "\n",
    "    # Duplicate the data n_repeats times\n",
    "    X_res = np.tile(X, (n_repeats, 1))\n",
    "    y_res = np.tile(y, n_repeats)\n",
    "\n",
    "    # Add Gaussian noise to the duplicated data\n",
    "    noise = np.random.normal(0, 0.1, size=X_res.shape)\n",
    "    X_res = X_res + noise\n",
    "\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebef3b34-6303-452d-9e47-a4a98adbb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic samples\n",
    "def generate_synthetic_samples(X, y, n_samples, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    nn = NearestNeighbors(n_neighbors=5)\n",
    "    nn.fit(X)\n",
    "\n",
    "    synthetic_X = []\n",
    "    synthetic_y = []\n",
    "    for _ in range(n_samples):\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        neighbors = nn.kneighbors([X[idx]], return_distance=False)[0]\n",
    "        \n",
    "        neighbor_idx = np.random.choice(neighbors)\n",
    "        lam = np.random.uniform(0, 1)\n",
    "        \n",
    "        # Generate synthetic sample using interpolation\n",
    "        new_sample_X = X[idx] + lam * (X[neighbor_idx] - X[idx])\n",
    "        new_sample_y = y[idx] + lam * (y[neighbor_idx] - y[idx])\n",
    "        \n",
    "        synthetic_X.append(new_sample_X)\n",
    "        synthetic_y.append(new_sample_y)\n",
    "    \n",
    "    return np.array(synthetic_X), np.array(synthetic_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafe4840-1bb1-4bcb-bf59-b5c146f74a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_samples_imbalanced(X, y, n_samples, threshold=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    nn = NearestNeighbors(n_neighbors=5)\n",
    "    nn.fit(X)\n",
    "\n",
    "    # Calculate distances to the nearest neighbors\n",
    "    distances, _ = nn.kneighbors(X)\n",
    "\n",
    "    # Calculate the number of neighbors within the threshold for each sample\n",
    "    neighbors_within_threshold = np.sum(distances[:, 1:] < threshold, axis=1)\n",
    "\n",
    "    # Calculate inverse probabilities\n",
    "    selection_probabilities = 1 / (neighbors_within_threshold + 1e-6)  # Add small constant to avoid division by zero\n",
    "    selection_probabilities /= selection_probabilities.sum()  # Normalize to sum to 1\n",
    "\n",
    "    synthetic_X = []\n",
    "    synthetic_y = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # Select a sample based on the weighted probability distribution\n",
    "        idx = np.random.choice(len(X), p=selection_probabilities)\n",
    "\n",
    "        # Select a random neighbor of the chosen sample\n",
    "        neighbors = nn.kneighbors([X[idx]], return_distance=False)[0]\n",
    "        neighbor_idx = np.random.choice(neighbors)\n",
    "\n",
    "        # Linear interpolation for synthetic sample generation\n",
    "        lam = np.random.uniform(0, 1)\n",
    "        new_sample_X = X[idx] + lam * (X[neighbor_idx] - X[idx])\n",
    "        new_sample_y = y[idx] + lam * (y[neighbor_idx] - y[idx])\n",
    "\n",
    "        synthetic_X.append(new_sample_X)\n",
    "        synthetic_y.append(new_sample_y)\n",
    "\n",
    "    return np.array(synthetic_X), np.array(synthetic_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9531828a-0be0-4345-aeae-eccae0aaf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_bootstrap(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Define the RandomForestRegressor model\n",
    "    rf_model = RandomForestRegressor(random_state=42, bootstrap=True)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],    # Number of trees in the forest\n",
    "        'max_depth': [5, 10, 20],            # Maximum depth of the tree\n",
    "        'min_samples_leaf': [1, 5, 10],      # Minimum number of samples required to be at a leaf node\n",
    "        'max_samples': [0.5, 0.7, 1.0],     # Maximum number of samples to draw from the data with replacement\n",
    "    }\n",
    "    \n",
    "    # Define the MAPE scorer (using Mean Absolute Percentage Error)\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "    \n",
    "    # Setup GridSearchCV to perform cross-validation\n",
    "    grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                                  scoring=mape_scorer, cv=5, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit the grid search to the duplicated training data\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Best hyperparameters from grid search\n",
    "    print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "    \n",
    "    # Best MAPE score from cross-validation\n",
    "    print(\"Best MAPE for Random Forest:\", -grid_search_rf.best_score_)\n",
    "    \n",
    "    # Train a final model using the best parameters\n",
    "    best_rf_model = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred_rf = best_rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the test MAPE\n",
    "    test_mape_rf = mape(y_test, y_pred_rf)\n",
    "    print(\"Test MAPE for Random Forest:\", test_mape_rf)\n",
    "\n",
    "    return best_rf_model, test_mape_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d9a109-29c2-4ae0-80c0-a8e0a6055d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_bootstrap_cv(X_train, X_test, y_train, y_test, cv=5):    \n",
    "    # Define the RandomForestRegressor model\n",
    "    rf_model = RandomForestRegressor(random_state=42, bootstrap=True)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],    # Number of trees in the forest\n",
    "        'max_depth': [5, 10, 20],           # Maximum depth of the tree\n",
    "        'min_samples_leaf': [1, 5, 10],     # Minimum samples required at a leaf node\n",
    "        'max_samples': [0.5, 0.7, 1.0],     # Maximum samples to draw from the data\n",
    "    }\n",
    "    \n",
    "    # Define the MAPE scorer (using Mean Absolute Percentage Error)\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "    \n",
    "    # Convert y_train to a 1D array using NumPy's ravel()\n",
    "    y_train_1d = np.ravel(y_train)\n",
    "\n",
    "    # Setup GridSearchCV to perform cross-validation\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        estimator=rf_model, \n",
    "        param_grid=param_grid, \n",
    "        scoring=mape_scorer, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search to the training data\n",
    "    grid_search_rf.fit(X_train, y_train_1d)\n",
    "    \n",
    "    # Get the best hyperparameters and best score from grid search\n",
    "    best_rf_model = grid_search_rf.best_estimator_\n",
    "    best_params_rf = grid_search_rf.best_params_\n",
    "    best_mape_rf = -grid_search_rf.best_score_\n",
    "    \n",
    "    print(\"Best Parameters for Random Forest:\", best_params_rf)\n",
    "    print(\"Best CV MAPE for Random Forest:\", best_mape_rf)\n",
    "    \n",
    "    # Perform cross-validation with the best model to get average MAPE\n",
    "    cv_mape_scores = cross_val_score(\n",
    "        best_rf_model, \n",
    "        X_train, \n",
    "        y_train_1d, \n",
    "        scoring=mape_scorer, \n",
    "        cv=cv, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    avg_cv_mape = -cv_mape_scores.mean()\n",
    "    \n",
    "    print(f\"Average CV MAPE for Random Forest: {avg_cv_mape}\")\n",
    "    \n",
    "    # Train the final model using the best parameters\n",
    "    best_rf_model.fit(X_train, y_train_1d)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred_rf = best_rf_model.predict(X_test)\n",
    "    test_mape_rf = mape(y_test, y_pred_rf)\n",
    "    print(\"Test MAPE for Random Forest:\", test_mape_rf)\n",
    "\n",
    "    return best_rf_model, avg_cv_mape, test_mape_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "112b498a-03a8-4fbe-abba-3e6d95ebd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_no_bootstrap(X_train, X_test, y_train, y_test):    \n",
    "    # Define the RandomForestRegressor model with bootstrap disabled\n",
    "    rf_model_no_bootstrap = RandomForestRegressor(random_state=42, bootstrap=False)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid_no_bootstrap = {\n",
    "        'n_estimators': [100, 200, 300],    # Number of trees in the forest\n",
    "        'max_depth': [3, 5, 10],            # Maximum depth of the tree\n",
    "        'min_samples_leaf': [1, 2, 4],      # Minimum number of samples required to be at a leaf node\n",
    "    }\n",
    "    \n",
    "    # Define the MAPE scorer (using Mean Absolute Percentage Error)\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "    \n",
    "    # Setup GridSearchCV to perform cross-validation\n",
    "    grid_search_rf_no_bootstrap = GridSearchCV(estimator=rf_model_no_bootstrap, param_grid=param_grid_no_bootstrap, \n",
    "                                               scoring=mape_scorer, cv=5, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit the grid search to the duplicated training data\n",
    "    grid_search_rf_no_bootstrap.fit(X_train, y_train)\n",
    "    \n",
    "    # Best hyperparameters from grid search\n",
    "    print(\"Best Parameters for Random Forest (No Bootstrap):\", grid_search_rf_no_bootstrap.best_params_)\n",
    "    \n",
    "    # Best MAPE score from cross-validation\n",
    "    print(\"Best MAPE for Random Forest (No Bootstrap):\", -grid_search_rf_no_bootstrap.best_score_)\n",
    "    \n",
    "    # Train a final model using the best parameters\n",
    "    best_rf_model_no_bootstrap = grid_search_rf_no_bootstrap.best_estimator_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred_rf_no_bootstrap = best_rf_model_no_bootstrap.predict(X_test)\n",
    "    \n",
    "    # Calculate the test MAPE\n",
    "    test_mape_rf_no_bootstrap = mape(y_test, y_pred_rf_no_bootstrap)\n",
    "    print(\"Test MAPE for Random Forest (No Bootstrap):\", test_mape_rf_no_bootstrap)\n",
    "\n",
    "    return best_rf_model_no_bootstrap, test_mape_rf_no_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04ff4e6d-ea63-444c-9c35-1df3ef29e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_no_bootstrap_cv(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \"\"\"\n",
    "    Trains a RandomForestRegressor model without bootstrap, using cross-validation\n",
    "    and GridSearchCV to find the best hyperparameters. Returns the best model,\n",
    "    average CV MAPE, and test MAPE.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Test features\n",
    "    - y_train: Training target\n",
    "    - y_test: Test target\n",
    "    - cv: Number of cross-validation folds (default: 5)\n",
    "\n",
    "    Returns:\n",
    "    - best_rf_model_no_bootstrap: Trained RandomForestRegressor model with best parameters\n",
    "    - avg_cv_mape_no_bootstrap: Average MAPE from cross-validation\n",
    "    - test_mape_rf_no_bootstrap: MAPE on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the RandomForestRegressor model with bootstrap disabled\n",
    "    rf_model_no_bootstrap = RandomForestRegressor(random_state=42, bootstrap=False)\n",
    "\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid_no_bootstrap = {\n",
    "        'n_estimators': [100, 200, 300],    # Number of trees in the forest\n",
    "        'max_depth': [3, 5, 10],            # Maximum depth of the tree\n",
    "        'min_samples_leaf': [1, 2, 4],      # Minimum samples required at a leaf node\n",
    "    }\n",
    "\n",
    "    # Define the MAPE scorer (using Mean Absolute Percentage Error)\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "    # Convert y_train to a 1D array using NumPy's ravel()\n",
    "    y_train_1d = np.ravel(y_train)\n",
    "\n",
    "    # Setup GridSearchCV to perform cross-validation\n",
    "    grid_search_rf_no_bootstrap = GridSearchCV(\n",
    "        estimator=rf_model_no_bootstrap, \n",
    "        param_grid=param_grid_no_bootstrap, \n",
    "        scoring=mape_scorer, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    grid_search_rf_no_bootstrap.fit(X_train, y_train_1d)\n",
    "\n",
    "    # Get the best hyperparameters and best score from grid search\n",
    "    best_rf_model_no_bootstrap = grid_search_rf_no_bootstrap.best_estimator_\n",
    "    best_params_rf_no_bootstrap = grid_search_rf_no_bootstrap.best_params_\n",
    "    best_cv_mape_no_bootstrap = -grid_search_rf_no_bootstrap.best_score_\n",
    "\n",
    "    print(\"Best Parameters for Random Forest (No Bootstrap):\", best_params_rf_no_bootstrap)\n",
    "    print(\"Best CV MAPE for Random Forest (No Bootstrap):\", best_cv_mape_no_bootstrap)\n",
    "\n",
    "    # Perform cross-validation with the best model to get average MAPE\n",
    "    cv_mape_scores_no_bootstrap = cross_val_score(\n",
    "        best_rf_model_no_bootstrap, \n",
    "        X_train, \n",
    "        y_train_1d, \n",
    "        scoring=mape_scorer, \n",
    "        cv=cv, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    avg_cv_mape_no_bootstrap = -cv_mape_scores_no_bootstrap.mean()\n",
    "\n",
    "    print(f\"Average CV MAPE for Random Forest (No Bootstrap): {avg_cv_mape_no_bootstrap}\")\n",
    "\n",
    "    # Train the final model using the best parameters\n",
    "    best_rf_model_no_bootstrap.fit(X_train, y_train_1d)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred_rf_no_bootstrap = best_rf_model_no_bootstrap.predict(X_test)\n",
    "    test_mape_rf_no_bootstrap = mape(y_test, y_pred_rf_no_bootstrap)\n",
    "    \n",
    "    print(\"Test MAPE for Random Forest (No Bootstrap):\", test_mape_rf_no_bootstrap)\n",
    "\n",
    "    return best_rf_model_no_bootstrap, avg_cv_mape_no_bootstrap, test_mape_rf_no_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee1349a-7bb1-4fc6-a759-4e923420a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    # Define the XGBoost model\n",
    "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],   # Number of trees\n",
    "        'max_depth': [5, 10, 20],            # Depth of the trees\n",
    "        'min_child_weight': [1, 5, 10],     # Minimum sum of instance weight (hessian)\n",
    "        'reg_lambda': [0.01, 0.1, 1, 10],  # L2 regularization term (lambda)\n",
    "        'reg_alpha': [0.01, 0.1, 1, 10],      # L1 regularization term (alpha)\n",
    "    }\n",
    "    \n",
    "    # Define the MAPE scorer (as we are optimizing based on Mean Absolute Percentage Error)\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "    \n",
    "    # Setup GridSearchCV to perform cross-validation\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                               scoring=mape_scorer, cv=5, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit the grid search to the duplicated training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best hyperparameters from grid search\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Best MAPE score from cross-validation\n",
    "    print(\"Best MAPE:\", -grid_search.best_score_)\n",
    "    \n",
    "    # Train a final model using the best parameters\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the test MAPE\n",
    "    test_mape = mape(y_test, y_pred)\n",
    "    print(\"Test MAPE:\", test_mape)\n",
    "\n",
    "    return best_xgb_model, test_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5edc8fce-3271-403a-a13b-ee0a6217d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_cv(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model with cross-validation, using GridSearchCV to find \n",
    "    the best hyperparameters, and returns the best model and average MAPE.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Test features\n",
    "    - y_train: Training target\n",
    "    - y_test: Test target\n",
    "    - cv: Number of cross-validation folds (default: 5)\n",
    "\n",
    "    Returns:\n",
    "    - best_xgb_model: The trained XGBoost model with the best parameters\n",
    "    - avg_cv_mape: Average MAPE from cross-validation\n",
    "    - test_mape: MAPE on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle missing values in X_train and y_train\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    y_train = y_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # Convert y_train to a 1D array using NumPy's ravel()\n",
    "    y_train_1d = np.ravel(y_train)\n",
    "\n",
    "    # Define the XGBoost model\n",
    "    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],   # Number of trees\n",
    "        'max_depth': [5, 10, 20],          # Depth of the trees\n",
    "        'min_child_weight': [1, 5, 10],    # Minimum sum of instance weight (hessian)\n",
    "        'reg_lambda': [0.01, 0.1, 1, 10],  # L2 regularization term (lambda)\n",
    "        'reg_alpha': [0.01, 0.1, 1, 10],   # L1 regularization term (alpha)\n",
    "    }\n",
    "\n",
    "    # Define the MAPE scorer (as we are optimizing based on Mean Absolute Percentage Error)\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "    # Setup GridSearchCV to perform cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model, \n",
    "        param_grid=param_grid, \n",
    "        scoring=mape_scorer, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train, y_train_1d)\n",
    "\n",
    "    # Get the best hyperparameters and best score from grid search\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_mape = -grid_search.best_score_\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best CV MAPE:\", best_cv_mape)\n",
    "\n",
    "    # Perform cross-validation with the best model to get average MAPE\n",
    "    cv_mape_scores = cross_val_score(\n",
    "        best_xgb_model, \n",
    "        X_train, \n",
    "        y_train_1d, \n",
    "        scoring=mape_scorer, \n",
    "        cv=cv, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    avg_cv_mape = -cv_mape_scores.mean()\n",
    "\n",
    "    print(f\"Average CV MAPE for XGBoost: {avg_cv_mape}\")\n",
    "\n",
    "    # Train the final model using the best parameters\n",
    "    best_xgb_model.fit(X_train, y_train_1d)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "    test_mape = mape(y_test, y_pred)\n",
    "    \n",
    "    print(\"Test MAPE for XGBoost:\", test_mape)\n",
    "\n",
    "    return best_xgb_model, avg_cv_mape, test_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47ef29e-7af3-4a90-ae7c-ef06f24092d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vape[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "              'dec_tobacco_supply', 'dec_smoking_uptake', 'average_age', \n",
    "              'gender_idx', 'ethnicity_idx']]\n",
    "y = df_vape[['qalys_pc']]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6, random_state=42)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Ensure that y is a 1D array for compatibility\n",
    "y_flat = y_train.values.flatten()\n",
    "\n",
    "# Generate synthetic samples with fixed random state\n",
    "X_res, y_res = simple_duplicate(X_train.values, y_flat, n_samples=200, random_state=42)\n",
    "\n",
    "# Stack the original and synthetic data\n",
    "X_full = np.vstack([X_train.values, X_res])\n",
    "y_full = np.hstack([y_flat, y_res])\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df_duplicated = pd.DataFrame(X_full, columns=X.columns)\n",
    "df_duplicated['qalys_pc'] = y_full\n",
    "\n",
    "X_duplicated = df_duplicated[X.columns]\n",
    "y_duplicated = df_duplicated['qalys_pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28791b7-2f54-4d25-a664-6f77914a049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vape[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "              'dec_tobacco_supply', 'dec_smoking_uptake', 'average_age', \n",
    "              'gender_idx', 'ethnicity_idx']]\n",
    "y = df_vape[['qalys_pc']]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Ensure that y is a 1D array for compatibility\n",
    "y_flat = y_train.values.flatten()\n",
    "\n",
    "# Generate synthetic samples with fixed random state\n",
    "X_res, y_res = generate_synthetic_samples(X_train.values, y_flat, n_samples=200, random_state=42)\n",
    "\n",
    "# Stack the original and synthetic data\n",
    "X_full = np.vstack([X_train.values, X_res])\n",
    "y_full = np.hstack([y_flat, y_res])\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df_resampled = pd.DataFrame(X_full, columns=X.columns)\n",
    "df_resampled['qalys_pc'] = y_full\n",
    "\n",
    "X_resampled = df_resampled[X.columns]\n",
    "y_resampled = df_resampled['qalys_pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a883f619-9fa1-4d42-b418-bb5c01d39972",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vape[['tax_increase', 'outlet_reduction', 'dec_smoking_prevalence', \n",
    "              'dec_tobacco_supply', 'dec_smoking_uptake', 'average_age', \n",
    "              'gender_idx', 'ethnicity_idx']]\n",
    "y = df_vape[['qalys_pc']]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Ensure that y is a 1D array for compatibility\n",
    "y_flat = y_train.values.flatten()\n",
    "\n",
    "# Generate synthetic samples with fixed random state\n",
    "X_res, y_res = generate_synthetic_samples_imbalanced(X_train.values, y_flat, n_samples=200, random_state=42)\n",
    "\n",
    "# Stack the original and synthetic data\n",
    "X_full = np.vstack([X_train.values, X_res])\n",
    "y_full = np.hstack([y_flat, y_res])\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df_resampled_imb = pd.DataFrame(X_full, columns=X.columns)\n",
    "df_resampled_imb['qalys_pc'] = y_full\n",
    "\n",
    "X_resampled_imb = df_resampled_imb[X.columns]\n",
    "y_resampled_imb = df_resampled_imb['qalys_pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d553f3-e14c-4484-825c-8e19b346557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters for Random Forest (No Bootstrap): {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Best CV MAPE for Random Forest (No Bootstrap): 0.08204362845518295\n",
      "Average CV MAPE for Random Forest (No Bootstrap): 0.08204362845518295\n",
      "Test MAPE for Random Forest (No Bootstrap): 0.801800817266545\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = rf_no_bootstrap_cv(X_duplicated, X_val, y_duplicated, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5aabc5c-c3e4-4bcc-b348-55a177dd2e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngjun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'max_depth': 20, 'max_samples': 1.0, 'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "Best CV MAPE for Random Forest: 0.19497555751389126\n",
      "Average CV MAPE for Random Forest: 0.19497555751389126\n",
      "Test MAPE for Random Forest: 0.900660094881084\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = rf_bootstrap_cv(X_duplicated, X_val, y_duplicated, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baacba03-7ce4-4a78-a1d4-458a5a439600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Best Parameters: {'max_depth': 20, 'min_child_weight': 1, 'n_estimators': 300, 'reg_alpha': 0.01, 'reg_lambda': 0.01}\n",
      "Best CV MAPE: 0.10452394633743749\n",
      "Average CV MAPE for XGBoost: 0.10452394633743749\n",
      "Test MAPE for XGBoost: 0.9403218538597944\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = xgboost_cv(X_duplicated, X_val, y_duplicated, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e850ab-078d-4847-9ed0-4214190086d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters for Random Forest (No Bootstrap): {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Best CV MAPE for Random Forest (No Bootstrap): 0.166558117409992\n",
      "Average CV MAPE for Random Forest (No Bootstrap): 0.166558117409992\n",
      "Test MAPE for Random Forest (No Bootstrap): 1.0757153163819957\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = rf_no_bootstrap_cv(X_resampled, X_val, y_resampled, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6400c9f7-e7f2-4356-ab00-a54bea27997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best Parameters for Random Forest: {'max_depth': 10, 'max_samples': 1.0, 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Best CV MAPE for Random Forest: 0.18953886909987502\n",
      "Average CV MAPE for Random Forest: 0.18953886909987502\n",
      "Test MAPE for Random Forest: 0.8648635291380432\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = rf_bootstrap_cv(X_resampled, X_val, y_resampled, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad77c2a-3df5-4aba-ae4b-8bc5078a4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Best Parameters: {'max_depth': 20, 'min_child_weight': 5, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 1}\n",
      "Best CV MAPE: 0.13481619814615212\n",
      "Average CV MAPE for XGBoost: 0.13481619814615212\n",
      "Test MAPE for XGBoost: 1.0446258915253233\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = xgboost_cv(X_resampled, X_val, y_resampled, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "677cd038-77f9-4d55-9f44-411e958105a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best Parameters for Random Forest: {'max_depth': 20, 'max_samples': 1.0, 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Best CV MAPE for Random Forest: 0.12192757260766973\n",
      "Average CV MAPE for Random Forest: 0.12192757260766973\n",
      "Test MAPE for Random Forest: 0.6949684664447683\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = rf_bootstrap_cv(X_resampled_imb, X_val, y_resampled_imb, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc408949-d579-47af-8b39-12cd2366e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters for Random Forest (No Bootstrap): {'max_depth': 10, 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Best CV MAPE for Random Forest (No Bootstrap): 0.13112286415678037\n",
      "Average CV MAPE for Random Forest (No Bootstrap): 0.13112286415678037\n",
      "Test MAPE for Random Forest (No Bootstrap): 0.5800002116955169\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = rf_no_bootstrap_cv(X_resampled_imb, X_val, y_resampled_imb, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4b052-0eea-43c6-b9e1-49dad65f2e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    }
   ],
   "source": [
    "best_rf_model, avg_cv_mape, test_mape_rf = xgboost_cv(X_resampled_imb, X_val, y_resampled_imb, y_val, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4945659-a501-42a6-abed-23fc21edc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfnb_model, rfnb_mape = rf_no_bootstrap(X_resampled_imb, X_val, y_resampled_imb, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1c69bc47-1fff-44b7-8d03-d5ee49b7479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfb_model, rfb_mape = rf_bootstrap(X_resampled_imb, X_val, y_resampled_imb, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d7f67e6b-7598-4864-a084-167d1a5fa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model, xgb_mape = xgboost(X_resampled_imb, X_val, y_resampled_imb, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "61f9aa36-b985-4671-a7b0-1725971acba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfnb_model, rfnb_mape = rf_no_bootstrap(X_resampled, X_val, y_resampled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "db49d848-fb5a-4aea-9eec-7990b91adb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfb_model, rfb_mape = rf_bootstrap(X_resampled, X_val, y_resampled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "678a7f76-6160-41bc-906e-d0272aaf8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model, xgb_mape = xgboost(X_resampled, X_val, y_resampled, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
